Title : Writeup for Project 4,Spring 2012
Date: 04/22/12
Group Num 3:  
Name                     Email                   StudentID
Shengzhe Chen     shengzhc@usc.edu       	4341396265
Pei Chu           peichu@usc.edu            	4097373598

I.Requirements:

1. Part1 of the project requires to simulate the communication between the server and client. Client must be user program and each client must be able to run up to 10 user program. Each user program uses system call to complete its own task, and the core of the client side nachos instance must get the service from server. Only 1 server is needed in this part.
2. Part2 of the project requires to implement fully distributed servers. In this part, there must exist more than 1 server. Each server can proceed the request from the client. Client-side is the same as part1. Servers must synchronize themselves correctly since client can send requests randomly.

II.Assumptions:

1. In this project, we remove the tlb from project3 and increase the NumPhysPages to a big enough size to eliminate page fault exception.
2. Make an assumption of the interaction between server and client. The interactive data is using in the below way. The first byte is the command, the hardcode defined request and reply value. For each request and reply, the client and server will acquire the function of each field in the corresponding data.
3. The protocol is defined in protocol.h in network directory.
4. When run in network, the server only allows at most 256(which defined in sersynch.h) client locks, conditions and mvs for client, and we assume its amount is sufficient so that we eliminate the destroy process, just replying the destroy reply back when destroy lock and condition. Besides, each lock, condition and mv has its own name within in its scope. It means, name is the ID for lock, condition and mv, respectively. While creating them, it should return the index if it has already existed, or return the index after allocating a new one.
5. As for the server lock which used in network sys calls, the lock owner we use is a integer value, which is identical. It is generated by the machine id * 100 + mail box id. 
6. For convenience, we assume that the server id should be from 1 to n where n can be specified by command line, we will discuss it later on testing section. Whereas, client and server must have different machine id.
7. Each client can have at most 20 processes, which is specified by MAX_PROCESSES, because we use assign each process a process id, and use this process id as the client mail box number.
8. While a server receives a service request, it promotes the msg to other servers first, and only after it receives the reply from other servers, it can address with this message.
9. For convenience, we specify the maximum number of entities in the doctor's office. The number of exam rooms are 5, X-ray rooms are 2, wrn are 1 and so on. Besides, it does not allow the user to input the entities number they want. If wanted, you could change the number in the loop in p1.c where, the entities except patient are created in p1.c, whereas p2.c is for child and parent, and p3.c is for adult patient. You can modify the number if needed by changing the loop times in these files, but notice that do not exceed the max processes one instance can have. 

III.Design:

Part I 
1. Data
In this part, it is only required to implement the client side program based on the use of  current server. In this case, we have to run up to 10 user program in a Nachos instance, thus, the mailboxes needed for client must be increased.
Besides, except for the elements required for client stored in Server, each client must have its own variable list so that each process can easily access to these variables.
2. Logic
The process flow is quite simple. Due to single server, clients send requests to only one server, and server can address with these request one by one, there is no race condition here and each client or each process has different id, identified by mailbox and machine id, so it is impossible to receive disordering reply msg from server. So the key is to manage the mail data so that server and client can parse the data correctly.

Part II
1. Data
In this part, except for the general message content, we need to add some elements to server and message. Timestamp is needed for each message between the communication among servers. Besides, server must have its own table to keep the updated info and a list of message received. 

2. Logic
The key in this part is to keep the consistency between servers, it means we must guarantee that each server must proceed the requests in the same order. Here, we import a extra field to indicate its unique key, which is increased through time. So we can easily get the proper message to address, which means this message is received early. In my case, each server has 2 threads, each thread listens to different socket or mailbox. When thread 1 get a message from client, it does not reply the msg directly. Instead, thread 1 must pend the message and enclose the message with its received time and then propagate to other servers. After other servers receive the message, it means other servers have added this message to their message queue, and then, now we can proceed this message. Although all the servers have this message, only one server can reply to client. The server which the client sends the message to must reply the message at some moment.  

IV.Implementation:

p.s the items modified in project2 are not listed here.

+Files Modified

exception.cc   addrspace.cc   addrspace.h   progtest.cc   syscall.h   main.cc  scheduler.cc  scheduler.h  system.cc  system.h

+Files added
network directory: client.cc   client.h   message.cc   message.h   server.cc   server.h   sersynch.cc  sersynch.h   protocol.h    
Test direcroty: p1.c   p2.c   p3.c wrn.c patient.c cashier.c doctor.c xray.c nurse.c parent.c child.c
userprog directory: mem.cc mem.h

+Data Structure added

sersynch.h:
class SLock {
    public:
    SLock(char *name);
    ~SLock();
    int Acquire(int presentowner);
    int Release(int presentowner);
    void Pend(Message *msg);
    Message *GetMsg();
    void SetOwner(int own) {owner = own;}
     char *GetName() {return lockname;}
 private:
    char *lockname;
    List *waitingList;
    int owner;
    bool status;
};

class SLockTable {
 public:
    SLockTable();
    ~SLockTable();
    SLock *GetLock(int index);
    int Acquire(int lockid);
    void Pend(int lockid, Message *msg);
    int Acquire(int lockid, int owner);
    int Release(int lockid, int owner);
    Message *GetMsg(int lockid);
    int SearchName(char *name);
    int AssignLock(char *name);
    int Destroy(int lockid);
    void SetLockOwner(int lockid, int owner);
private:
    BitMap *slmap;
    SLock **sltable;
};

class SCondition {
public:
    SCondition(char *name);
    ~SCondition();
    char *GetName() {return scname;}
    void Pend(Message *msg);
    int Wait(int lockid);
    int Signal(int lockid);
    Message* GetMsg();
private:
    char *scname;
    List *waitingList;
    SLock *waitingLock;
};

class SCTable {
 public:
    SCTable();
    ~SCTable();
    Message* GetMsg(int cvid);
    void Pend(int cvid, Message *msg);
    int SearchName(char *name);
    int AssignCondition(char *name);
    int Destroy(int cvid);
    int Wait(int cvid, int lockid);
    int Signal(int cvid, int lockid);
private:
    BitMap *scmap;
    SCondition **sctable;
};

class MV
{
public:
    MV(char *name, int size) {mvname = name; length = size; mv = new int[size];}
    ~MV() {delete mvname; delete mv;}
    char *GetName() {return mvname;}
    int SetValue(int index, int value);
    int GetValue(int index);
private:
    char *mvname;
    int *mv;
    int length;
};

class MVTable {
public:
    MVTable();
    ~MVTable();
    int SearchName(char *name);
    int AssignMV(char *name, int size);
    int SetMV(int mvid, int index, int value);
    int GetMV(int mvid, int index);
    int DestroyMV(int mvid);
private:
    BitMap *mvmap;
    MV **mvtable;
};

+Data Structure modified

nothing

+Functions added

client.cc:
    Client(NetworkAddress addr, double reliability, int nBoxes);
    ~Client();
    void Send(char *, NetworkAddress sid, int cmid, int length);
    void Receive(char **, int cmid);


message.cc:
    Message(PacketHeader phdr, MailHeader mhdr, char *buffer);
    Message(PacketHeader phdr, MailHeader mhdr, char *buffer, int64_t t);
    void SetTime(int64_t t) {timestamp = t;}
    ~Message() {}

post.cc:
   PostOffice::PostOffice(NetworkAddress addr, double reliability, int nBoxes);
   PostOffice::~PostOffice();		    
    bool Send(PacketHeader pktHdr, MailHeader mailHdr, char *data);
    				
    void Receive(int box, PacketHeader *pktHdr, MailHeader *mailHdr, char *data);
    void PostalDelivery();	
    void PacketSent();		
    void IncomingPacket();

server.cc:
   Server(NetworkAddress addr, double reliability, int nBoxes, int volum);
    ~Server();
    void Run();
    void Reply(Message *msg);
    void Print();
    void CreateLockReply(PacketHeader outPktHdr, MailHeader outMailHdr, char *data, bool reply);
    void DestroyLockReply(int lockid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void AcquireLockReply(int lockid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void ReleaseLockReply(int lockid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void CreateConditionReply(PacketHeader outPktHdr, MailHeader outMailHdr, char *in, bool reply);
    void DestroyConditionReply(int cvid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void WaitConditionReply(int cvid, int lockid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void SignalConditionReply(int cvid, int lockid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void BroadcastConditionReply(int cvid, int lockid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void CreateMVReply(PacketHeader outPktHdr, MailHeader outMailHdr, char *in, bool reply);
    void SetMVReply(int mvid, int index, int value, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void GetMVReply(int mvid, int index, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void DestroyMVReply(int mvid, PacketHeader outPktHdr, MailHeader outMailHdr, bool reply);
    void Propagate();
    void ReceiptThread();
    bool IsUpdated();
    void ProcessdMsg();
	

sersynch.cc:
   MVTable::MVTable();
    MVTable::~MVTable();
    int SearchName(char *name);
    int AssignMV(char *name, int size);
    int SetMV(int mvid, int index, int value);
    int GetMV(int mvid, int index);
    int DestroyMV(int mvid);

   MV::MV(char *name, int size) {mvname = name; length = size; mv = new int[size];}
    MV::~MV() {delete mvname; delete mv;}
    char *GetName() {return mvname;}
    int SetValue(int index, int value);
    int GetValue(int index);
 
  SCTable::SCTable();
    SCTable::~SCTable();
    Message* GetMsg(int cvid);
    void Pend(int cvid, Message *msg);
    int SearchName(char *name);
    int AssignCondition(char *name);
    int Destroy(int cvid);
    int Wait(int cvid, int lockid);
    int Signal(int cvid, int lockid);

  SCondition::SCondition(char *name);
   SCondition:: ~SCondition();
    char *GetName() {return scname;}
    void Pend(Message *msg);
    int Wait(int lockid);
    int Signal(int lockid);
    Message* GetMsg();
   
   SLockTable::SLockTable();
    SLockTable::~SLockTable();
    SLock *GetLock(int index);
    int Acquire(int lockid);
    void Pend(int lockid, Message *msg);
    int Acquire(int lockid, int owner);
    int Release(int lockid, int owner);
    Message *GetMsg(int lockid);
    int SearchName(char *name);
    int AssignLock(char *name);
    int Destroy(int lockid);
    void SetLockOwner(int lockid, int owner);

    SLock::SLock(char *name);
    SLock::~SLock();
    int Acquire(int presentowner);
    int Release(int presentowner);
    void Pend(Message *msg);
    Message *GetMsg();
    void SetOwner(int own) {owner = own;}

TranslationEntry *AddrSpace::SearchEntry(int vaddr)   --------addrspace.cc
void TerminateProcess()    -----------addrspace.cc
int CreateMV_Syscall(unsigned int vaddr, int length, int size);  ---addrspace.cc
int SetMV_Syscall(int mvid, int index, int value);      -----addrspace.cc
int GetMV_Syscall(int mvid, int index);      -------addrspace.cc
int DestroyMV_Syscall(int mvid);     -------addrspace.cc

int Scheduler::ReadyCount();    ---------scheduler.cc

+Functions modified

addrspace.cc:

int AddrSpace::AllocateNewStack() ; 
int AddrSpace::DeallocateStack() ;

exception.cc:
int CreateLock_Syscall(unsigned int vaddr, int length) ;
int DestroyLock_Syscall(int lockid);
int AcquireLock_Syscall(int lockid);
int ReleaseLock_Syscall(int lockid);
int CreateCondition_Syscall(unsigned int vaddr, int length);
int DestroyCondition_Syscall(int cvid);
int WaitCondition_Syscall(int cvid, int lockid);
int SignalCondition_Syscall(int cvid, int lockid);
int BroadcastCondition_Syscall(int cvid, int lockid);
void Exit_Syscall(int status);
int Exec_Syscall(unsigned int vaddr, int length); 
void ExceptionHandler(ExceptionType which) ;

progtest.cc

StartProcess(char *filename)  -------progtest.cc

main.cc
int main(int argc, char **argv) -------main.cc

system.cc
void Initialize(int argc, char **argv) ----system.cc
void Cleanup(); ----system.cc

V.Testing:
1. one server test

For 1 server test, you have to open 4 terminals and type the command as below
N1: nachos -sid 1 -cap 1
N2: nachos -x ../test/p1 -m 6 -cap 1
N3: nachos -x ../test/p2 -m 7 -cap 1
N4: nachos -x ../test/p3 -m 8 -cap 1

"-sid" assigns a machine id to the server.
"-cap" is to tell the server how many servers are in the system and tell the client 
"-m" assigns a machine id to client.

In this part, 4 nachos instances are running. N1 is the server, and N2 has 1 wrn, 5 nurses, 2 X-ray technicians, 1 cashier, 2 doctors. N3 has 5 children and 5 parents, N4 has 8 adult patients. As you can see after you run the command, the terminals will display the details of the processing flow. Besides in the Server's terminal, it will display the messages they receive and reply. You can find that the server will not stop because N1 keeps running all the time. 

2. fully distributed server test

For fully distributed server test, you can open whatever you like to be the quantities of server. Here we open 5 servers and 3 clients, each client runs about 10 user program.
N1: nachos -sid 1 -cap 5
N2: nachos -sid 2 -cap 5
N3: nachos -sid 3 -cap 5
N4: nachos -sid 4 -cap 5
N5: nachos -sid 5 -cap 5
N6: nachos -x ../test/p1 -m 6 -cap 5
N7: nachos -x ../test/p2 -m 7 -cap 5
N8: nachos -x ../test/p3 -m 8 -cap 5

"-sid" assigns a machine id to the server.
"-cap" is to tell the server how many servers are in the system and tell the client 
"-m" assigns a machine id to client.

Here, we run 5 servers, and servers are connected. 3 Clients are full of the role processes of doctor's office. 

VI.Discussion:

+Experiment expectation

1. one server test
In this experiment, finally we could find that all adult patients and parents will leave the doctor's office. You can easily search by the word "leaves" to highlight all patients have gone. 

2. fully distribute server test
In this experiment, we should see that all the patient should leave the doctor's office ultimately, and the servicing sequences are correct.

+Experiment Result

Just as same as the experiment expectation.

+Explanation

VII.Miscellaneous:

1. Here we are not explicitly using destroyxxx system calls. When using such system calls, it will delete the corresponding object in server. So we do not implement the strategy to keep them, but the quantity is enough for allocating server lock, condition and mv.
2. Sometimes when we run the user client(patient client), one of the clients may not halt themselves but all the patients have gone. It should be further considered.
3. If you run lots of adult patient at the same time, it might disorder the sequence of waiting room nurse. I try to figure out a solution, but the reason for this problem may be the timestamp which is insufficient to guarantee the order between multi servers. 
